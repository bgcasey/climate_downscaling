```{r setup, include=FALSE, cache=FALSE}
#Set root directory to R project root
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
```



``` {r eval=FALSE}
library(raster)
library(dismo)
library(gbm)
library(lme4)
library(dplyr)
library(caret)        # an aggregator package for performing many machine learning models

load("0_data/manual/for_models/data_full.rData")
df<-data_full%>%
  mutate(season_2=as.factor(season_2))
```

### Check for multicollinearity between predictors {-}


```{r}
pairs_cov <-data_full[c(14:39, 42)]

#visualize with corrplot. Easier to visualize with a lot of variables
M<-cor(pairs_cov, method = "pearson", use="pairwise.complete.obs")
corrplot(M, tl.cex=0.5, method="number", type ="upper", addCoefasPercent=TRUE, order = "hclust", number.cex=.5, cl.cex=.5
)
```



### BRT {-}

Used https://rspatial.org/raster/sdm/9_sdm_brt.html as a referenced





#### Split data into training and test sets {-}

```{r eval=FALSE}
set.seed(3456)
samp <- sample(nrow(df), round(0.75 * nrow(df)))
train_data <- df[samp,]
save(train_data, file="2_pipeline/store/train_data.rData")
# traindata <- traindata[traindata[,1] == 1, 2:9]
test_data <- df[-samp,]
save(test_data, file="2_pipeline/store/test_data.rData")

```

#### Tune BRT parameters {-}

Tutorials on tuning:
- https://uc-r.github.io/gbm_regression
- Kuhn, M., & Johnson, K. (2013). Applied predictive modeling (Vol. 26, p. 13). New York: Springer.

```{r eval=FALSE}
# gbmGrid <- expand.grid(.interaction.depth = seq(1, 7, by = 2),
#                     .n.trees = seq(100, 1000, by = 50),
#                     .shrinkage = c(0.01, 0.1),
#                     .n.minobsinnode=c(10, 20))
# 
# 
# nrow(gbmGrid)
# 
# set.seed(100)
# 
# gbmTune <- train(x=as.data.frame(df[c(3, 14, 16, 21:24, 26:35, 38, 40, 43:45, 47)]), y=test$Tavg_diff,
#                    method = "gbm",
#                    tuneGrid = gbmGrid,
#                    ## The gbm() function produces copious amounts
#                    ## of output, so pass in the verbose option
#                    ## to avoid printing a lot to the screen.
#                    verbose = FALSE)




# create hyperparameter grid
hyper_grid <- expand.grid(
  shrinkage = c(.001, .01, .1),
  interaction.depth = c(2, 3, 5),
  n.trees = seq(100, 1000, by = 50),
  n.minobsinnode = c(5, 10, 15),
  bag.fraction = c(.5, .75), 
  optimal_trees = 0,               # a place to dump results
  min_RMSE = 0                 # a place to dump results
)

# total number of combinations
nrow(hyper_grid)
## [1] 1026


df1<-df[c(13,3, 14, 16, 21:24, 26:35, 38, 40, 43:45, 47)]

# split and randomize data

set.seed(3456)
samp <- sample(nrow(df1), round(0.70 * nrow(df)))
train_data <- df1[samp,]
save(train_data, file="2_pipeline/store/train_data.rData")
# traindata <- traindata[traindata[,1] == 1, 2:9]
test_data <- df1[-samp,]
save(test_data, file="2_pipeline/store/test_data.rData")



# grid search 
for(i in 1:nrow(hyper_grid)) {
  
  # reproducibility
  set.seed(123)
  
  # train model
  gbm.tune <- gbm(
    formula = Tavg_diff ~ .,
    distribution = "gaussian",
    data = train_data,
    n.trees = hyper_grid$n.trees[i],
    interaction.depth = hyper_grid$interaction.depth[i],
    shrinkage = hyper_grid$shrinkage[i],
    n.minobsinnode = hyper_grid$n.minobsinnode[i],
    bag.fraction = hyper_grid$bag.fraction[i],
    train.fraction = .75,
    n.cores = NULL, # will use all cores by default
    verbose = FALSE
  )
  
  # add min training error and trees to grid
  hyper_grid$optimal_trees[i] <- which.min(gbm.tune$valid.error)
  hyper_grid$min_RMSE[i] <- sqrt(min(gbm.tune$valid.error))
}

hyper_grid_2<-hyper_grid %>% 
  mutate(min_NRMSE=min_RMSE/max(train_data$Tavg_diff)-min(train_data$Tavg_diff))%>%
  dplyr::arrange(min_RMSE) 


%>%
  head(10)

hyper_grid<-tune_param_1
tune_param_1<-hyper_grid
save(tune_param_1, file="2_pipeline/store/tune_param_1.rData")

```


#### Fit BRT models {-}

###### Difference between mean temperature {-}


```{r eval=FALESE}

# Build initial BRT using the gbm.step function
brt_meanTemp_1 <- gbm.step(data=df, gbm.x = c(3, 14, 16, 21:24, 26:35, 38, 40, 43:45, 47), gbm.y = 13,
                        family = "gaussian", tree.complexity = 2,
                        learning.rate = 0.001, bag.fraction = 0.5, max.trees = 100000)
save(brt_meanTemp_1, file="2_pipeline/store/models/brt_meanTemp_1.rData")

gbm.plot(brt_meanTemp_1, n.plots=21, write.title = FALSE)


brt_meanTemp_2 <- gbm.step(data=df, gbm.x = c(3, 14, 16, 21:24, 26:35, 38, 40, 43:45, 47), gbm.y = 13,
                        family = "gaussian", tree.complexity = 2, n.trees = 500,
                        learning.rate = 0.001, bag.fraction = 0.5)
# save(brt_meanTemp_1, file="2_pipeline/store/models/brt_meanTemp_1.rData")


brt_meanTemp_3 <- gbm.step(data=df, gbm.x = c(3, 14, 16, 21:24, 26:35, 38, 40, 43:45, 47), gbm.y = 13,
                        family = "gaussian", tree.complexity = 2, n.trees = 750,
                        learning.rate = 0.001, bag.fraction = 0.5)
# save(brt_meanTemp_1, file="2_pipeline/store/models/brt_meanTemp_1.rData")
#10500


brt_meanTemp_4 <- gbm.step(data=df, gbm.x = c(3, 14, 16, 21:24, 26:35, 38, 40, 43:45, 47), gbm.y = 13,
                        family = "gaussian", tree.complexity = 2, n.trees = 1000,
                        learning.rate = 0.001, bag.fraction = 0.5)
# save(brt_meanTemp_1, file="2_pipeline/store/models/brt_meanTemp_1.rData")
#flag


brt_meanTemp_5 <- gbm.step(data=df, gbm.x = c(3, 14, 16, 21:24, 26:35, 38, 40, 43:45, 47), gbm.y = 13,
                        family = "gaussian", tree.complexity = 2, n.trees = 1500,
                        learning.rate = 0.001, bag.fraction = 0.5)
# save(brt_meanTemp_1, file="2_pipeline/store/models/brt_meanTemp_1.rData")

## Identify and eliminate unimportant variables. Drop variables that don't improve model performance. 

simp_meanTemp <- gbm.simplify(brt_meanTemp_1)
save(simp_meanTemp, file="2_pipeline/store/models/simp_meanTemp.rData")

# get optimal number of drops from gbm.simp_meanTemplify models

##  remove non-numeric characters from the row names
rownames(simp_meanTemp$deviance.summary) <- gsub("[^0-9]", "", rownames(simp_meanTemp$deviance.summary))

## get the optimal number of drops
optimal_no_drops <-as.numeric(rownames(simp_meanTemp$deviance.summary%>%slice_min(mean))) 


# Build a new model with variables selected from gbm.simplify
# brt_meanTemp_2 <- gbm.step(data=df, gbm.x = simp_meanTemp$pred.list[[optimal_no_drops]], gbm.y = 13,
#                         family = "gaussian", tree.complexity = 5,
#                         learning.rate = 0.001, bag.fraction = 0.75, max.trees = 10000)

brt_meanTemp_2 <- gbm.step(data=df, gbm.x = simp_meanTemp$pred.list[[optimal_no_drops]], gbm.y = 13,
                        family = "gaussian", tree.complexity = 2,  max.trees = 100000,
                        learning.rate = 0.001, bag.fraction = 0.75)

#38450
save(brt_meanTemp_2, file="2_pipeline/store/models/brt_meanTemp_2.rData")


# find interactions
interactions_meanTemp<-gbm.interactions(brt_meanTemp_2)
save(interactions_meanTemp, file="2_pipeline/store/models/interactions_meanTemp.rData")

# interactions_meanTemp$rank.list
# interactions_meanTemp$interactions
## plot interactions
# gbm.perspec(brt_meanTemp_2, 22, 9, y.range=c(15,20), z.range=c(-600,1200))


## From https://uc-r.github.io/gbm_regression#xgboost

# Visualze variable imprortance
library(vip)
pdf("3_output/figures/BRTs/brt_meanTemp_2_variable_importance.pdf")
  vip::vip(brt_meanTemp_2, num_features=25L)
dev.off()

# Partial dependence plots
pdf("3_output/figures/BRTs/brt_meanTemp_2_partial_dependence_plots.pdf")
    gbm.plot(brt_meanTemp_2, n.plots=22,smooth=TRUE)
dev.off()

# put relevant stats into a dataframe (e.g. explained deviance)
varimp.brt_meanTemp_2 <- as.data.frame(brt_meanTemp_2$contributions)
names(varimp.brt_meanTemp_2)[2] <- "brt_meanTemp_2"
cvstats.brt_meanTemp_2 <- as.data.frame(brt_meanTemp_2$cv.statistics[c(1,3)])
cvstats.brt_meanTemp_2$deviance.null <- brt_meanTemp_2$self.statistics$mean.null
cvstats.brt_meanTemp_2$deviance.explained <- (cvstats.brt_meanTemp_2$deviance.null-cvstats.brt_meanTemp_2$deviance.mean)/cvstats.brt_meanTemp_2$deviance.null


varimp.brt_m2 <- as.data.frame(m2$contributions)
names(varimp.brt_m2)[2] <- "brt_m2"
cvstats.brt_m2 <- as.data.frame(m2$cv.statistics[c(1,3)])
cvstats.brt_m2$deviance.null <- m2$self.statistics$mean.null
cvstats.brt_m2$deviance.explained <- (cvstats.brt_m2$deviance.null-cvstats.brt_m2$deviance.mean)/cvstats.brt_m2$deviance.null
```

##### Difference between max temperature {-}

```{r eval=FALESE}

# Build initial BRT using the gbm.step function
brt_maxTemp_1 <- gbm.step(data=df, gbm.x = c(3, 14, 16, 21:24, 26:35, 38, 40, 43:45, 47), gbm.y = 11,
                        family = "gaussian", tree.complexity = 5,
                        learning.rate = 0.001, bag.fraction = 0.75, max.trees = 100000)
save(brt_maxTemp_1, file="2_pipeline/store/models/brt_maxTemp_1.rData")

gbm.plot(brt_maxTemp_1, n.plots=21, write.title = FALSE)



## Identify and eliminate unimportant variables. Drop variables that don't improve model performance. 

simp_maxTemp <- gbm.simplify(brt_maxTemp_1)
save(simp_maxTemp, file="2_pipeline/store/models/simp_maxTemp.rData")

# get optimal number of drops from gbm.simp_maxTemplify models

##  remove non-numeric characters from the row names
rownames(simp_maxTemp$deviance.summary) <- gsub("[^0-9]", "", rownames(simp_maxTemp$deviance.summary))

## get the optimal number of drops
optimal_no_drops <-as.numeric(rownames(simp_maxTemp$deviance.summary%>%slice_min(max))) 


# Build a new model with variables selected from gbm.simplify
# brt_maxTemp_2 <- gbm.step(data=df, gbm.x = simp_maxTemp$pred.list[[optimal_no_drops]], gbm.y = 13,
#                         family = "gaussian", tree.complexity = 5,
#                         learning.rate = 0.001, bag.fraction = 0.75, max.trees = 10000)

brt_maxTemp_2 <- gbm.step(data=df, gbm.x = simp_maxTemp$pred.list[[optimal_no_drops]], gbm.y = 11,
                        family = "gaussian", tree.complexity = 5,  max.trees = 100000,
                        learning.rate = 0.001, bag.fraction = 0.75)

#38450
save(brt_maxTemp_2, file="2_pipeline/store/models/brt_maxTemp_2.rData")


# find interactions
interactions_maxTemp<-gbm.interactions(brt_maxTemp_2)
save(interactions_maxTemp, file="2_pipeline/store/models/interactions_maxTemp.rData")

# interactions_maxTemp$rank.list
# interactions_maxTemp$interactions
## plot interactions
# gbm.perspec(brt_maxTemp_2, 22, 9, y.range=c(15,20), z.range=c(-600,1200))


## From https://uc-r.github.io/gbm_regression#xgboost

# Visualze variable imprortance
library(vip)
pdf("3_output/figures/BRTs/brt_maxTemp_2_variable_importance.pdf")
  vip::vip(brt_maxTemp_2, num_features=25L)
dev.off()

# Partial dependence plots
pdf("3_output/figures/BRTs/brt_maxTemp_2_partial_dependence_plots.pdf")
    gbm.plot(brt_maxTemp_2, n.plots=22,smooth=TRUE)
dev.off()

# put relevant stats into a dataframe (e.g. explained deviance)
varimp.brt_maxTemp_2 <- as.data.frame(brt_maxTemp_2$contributions)
names(varimp.brt_maxTemp_2)[2] <- "brt_maxTemp_2"
cvstats.brt_maxTemp_2 <- as.data.frame(brt_maxTemp_2$cv.statistics[c(1,3)])
cvstats.brt_maxTemp_2$deviance.null <- brt_maxTemp_2$self.statistics$max.null
cvstats.brt_maxTemp_2$deviance.explained <- (cvstats.brt_maxTemp_2$deviance.null-cvstats.brt_maxTemp_2$deviance.max)/cvstats.brt_maxTemp_2$deviance.null


varimp.brt_m2 <- as.data.frame(m2$contributions)
names(varimp.brt_m2)[2] <- "brt_m2"
cvstats.brt_m2 <- as.data.frame(m2$cv.statistics[c(1,3)])
cvstats.brt_m2$deviance.null <- m2$self.statistics$max.null
cvstats.brt_m2$deviance.explained <- (cvstats.brt_m2$deviance.null-cvstats.brt_m2$deviance.max)/cvstats.brt_m2$deviance.null
```


##### Difference between min temperature {-}


```{r eval=FALESE}

# Build initial BRT using the gbm.step function
brt_minTemp_1 <- gbm.step(data=df, gbm.x = c(3, 14, 16, 21:24, 26:35, 38, 40, 43:45, 47), gbm.y = 12,
                        family = "gaussian", tree.complexity = 5,
                        learning.rate = 0.001, bag.fraction = 0.75, max.trees = 100000)
save(brt_minTemp_1, file="2_pipeline/store/models/brt_minTemp_1.rData")

gbm.plot(brt_minTemp_1, n.plots=21, write.title = FALSE)



## Identify and eliminate unimportant variables. Drop variables that don't improve model performance. 

simp_minTemp <- gbm.simplify(brt_minTemp_1)
save(simp_minTemp, file="2_pipeline/store/models/simp_minTemp.rData")

# get optimal number of drops from gbm.simp_minTemplify models

##  remove non-numeric characters from the row names
rownames(simp_minTemp$deviance.summary) <- gsub("[^0-9]", "", rownames(simp_minTemp$deviance.summary))

## get the optimal number of drops
optimal_no_drops <-as.numeric(rownames(simp_minTemp$deviance.summary%>%slice_min(min))) 


# Build a new model with variables selected from gbm.simplify
# brt_minTemp_2 <- gbm.step(data=df, gbm.x = simp_minTemp$pred.list[[optimal_no_drops]], gbm.y = 13,
#                         family = "gaussian", tree.complexity = 5,
#                         learning.rate = 0.001, bag.fraction = 0.75, max.trees = 10000)

brt_minTemp_2 <- gbm.step(data=df, gbm.x = simp_minTemp$pred.list[[optimal_no_drops]], gbm.y = 12,
                        family = "gaussian", tree.complexity = 5,  max.trees = 100000,
                        learning.rate = 0.001, bag.fraction = 0.75)

#38450
save(brt_minTemp_2, file="2_pipeline/store/models/brt_minTemp_2.rData")


# find interactions
interactions_minTemp<-gbm.interactions(brt_minTemp_2)
save(interactions_minTemp, file="2_pipeline/store/models/interactions_minTemp.rData")

# interactions_minTemp$rank.list
# interactions_minTemp$interactions
## plot interactions
# gbm.perspec(brt_minTemp_2, 22, 9, y.range=c(15,20), z.range=c(-600,1200))


## From https://uc-r.github.io/gbm_regression#xgboost

# Visualze variable imprortance
library(vip)
pdf("3_output/figures/BRTs/brt_minTemp_2_variable_importance.pdf")
  vip::vip(brt_minTemp_2, num_features=25L)
dev.off()

# Partial dependence plots
pdf("3_output/figures/BRTs/brt_minTemp_2_partial_dependence_plots.pdf")
    gbm.plot(brt_minTemp_2, n.plots=22,smooth=TRUE)
dev.off()

# put relevant stats into a dataframe (e.g. explained deviance)
varimp.brt_minTemp_2 <- as.data.frame(brt_minTemp_2$contributions)
names(varimp.brt_minTemp_2)[2] <- "brt_minTemp_2"
cvstats.brt_minTemp_2 <- as.data.frame(brt_minTemp_2$cv.statistics[c(1,3)])
cvstats.brt_minTemp_2$deviance.null <- brt_minTemp_2$self.statistics$min.null
cvstats.brt_minTemp_2$deviance.explained <- (cvstats.brt_minTemp_2$deviance.null-cvstats.brt_minTemp_2$deviance.min)/cvstats.brt_minTemp_2$deviance.null


varimp.brt_m2 <- as.data.frame(m2$contributions)
names(varimp.brt_m2)[2] <- "brt_m2"
cvstats.brt_m2 <- as.data.frame(m2$cv.statistics[c(1,3)])
cvstats.brt_m2$deviance.null <- m2$self.statistics$min.null
cvstats.brt_m2$deviance.explained <- (cvstats.brt_m2$deviance.null-cvstats.brt_m2$deviance.min)/cvstats.brt_m2$deviance.null
```














##### Combine results into a single dataframe {-}

```{r eval=FALSE}
# cvstats
  cvstats.combo <- rbind(cvstats.nbr12,cvstats.nbr3,cvstats.nbr5,cvstats.ndvi12,cvstats.ndvi3,cvstats.ndvi5)
  cvstats.combo <- cbind(metrics,cvstats.combo)
  write.csv(cvstats.combo, file=paste0(w,"yukon_eco",ecozones[i],"cvstats.csv"))

  
# Variable importance    
  varimp.combo <- inner_join(varimp.nbr12,varimp.nbr3,by="var")
  varimp.combo <- inner_join(varimp.combo,varimp.nbr5,by="var")
  varimp.combo <- inner_join(varimp.combo,varimp.ndvi12,by="var")  
  varimp.combo <- inner_join(varimp.combo,varimp.ndvi3,by="var") 
  varimp.combo <- inner_join(varimp.combo,varimp.ndvi5,by="var")  
  write.csv(varimp.combo, file=paste0(w,"yukon_eco",ecozones[i],"varimp.csv"))
  
varimp <- read.csv(paste0(w,"yukon_ecor",ecoregions[1],"varimp.csv"))
for (i in 2:length(ecoregions)) {
  varimp1 <- read.csv(paste0(w,"yukon_ecor",ecoregions[i],"varimp.csv"))
  varimp <- rbind(varimp,varimp1)
}
varimpsum <- aggregate(varimp[,3:8],by=list(varimp$var),FUN="sum")
write.csv(varimpsum,file=paste0(w,"yukon_varimpsum.csv"))  
  
```



#### Spatial prediction {-}

##### Load spatial variable rasters {-}

There is no raster data for month and season so we'll create a data frame with a constant value to plug into the predict function.

```{r}
Method <- factor('electric', levels = levels(Anguilla_train$Method))
add <- data.frame(Method)
```



##### Create predictive rasters {-}

###### Difference between mean temperature {-}

```{r eval=FALSE}
p <- predict(Anguilla_grids, angaus.tc5.lr005, const=add,
       n.trees=angaus.tc5.lr005$gbm.call$best.trees, type="response")
p <- mask(p, raster(Anguilla_grids, 1))
plot(p, main='Angaus - BRT prediction')
```




###### Difference between max temperature {-}

###### Difference between min temperature {-}







#### Predict to test data {-}

```{r eval=FALSE}
load("2_pipeline/store/test_data.rData")

preds <- predict.gbm(m2, test_data,
         n.trees=m2$gbm.call$best.trees, type="response")

# get MSE and compute RMSE
sqrt(min(m2$cv.values))

caret::RMSE(preds,test_data$Tavg_diff)
#0.6987317

#visualize predictions vs test data
x_ax = 1:length(preds)
plot(x_ax, test_data$Tavg_diff, col="blue", pch=20, cex=.9)
lines(x_ax, preds, col="red", pch=20, cex=.9) 


calc.deviance(obs=test_data$Tavg_diff, pred=preds, calc.mean=TRUE)

d <- cbind(test_data$Tavg_diff, preds)
pres <- d[d[,1]==1, 2]
abs <- d[d[,1]==0, 2]
e <- evaluate(p=pres, a=abs)
e

df$test<-df$V1
df<-as.data.frame(d
                  )

library(cvms)
e <- evaluate(df, target_col=test, prediction_cols =  preds)




```

#### Compare predictions with data {-}





